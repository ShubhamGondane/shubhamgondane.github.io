---
title: 'Learning hyperparameter η for Proximal Policy Option-Critic(PPOC) Algorithm Applied To Continuous Action Tasks'

permalink: /projects/ppoc
tags:
  - Planning and Learning
  - Artificial Intelligence
  - Reinforcement Learning
  - OpenAI Gym
driveId1: 1zIUueEWeAM3lvAZ5S-qXTM8y8EMkZL5X/preview
driveId2: 1IIA071X28tqwY61HPKJ3ZkcJuoFHlpGX/preview
---



The options framework allows a reinforcement learning agent to represent, learn and plan with temporally extended actions.  

Given a set of options, the agent will learn a policy over options, which is typically viewed as executing in a call-and-return fashion: once this policy chooses an option, the option will execute until it terminates, then the policy over options will make a new choice. Learning options is beneficial as it leads to specialization in the state space, and therefore to potentially reduced complexity in terms of the internal policies of the options.

The option-critic architecture provides an agent with an end-to-end algorithm to learn options in order to maximize the expected discounted return, by relying on ideas akin to actor-critic methods.
The option-critic architecture is combined with Proximal Policy Optimization (PPO), which is very well suited for continuous control tasks and has shown better sample complexity in empirical comparisons

We build on the results of the Proximal Policy Options Critic algorithm for continuous control tasks and aim to learn the characteristics of the deliberation cost (η) that controls selection of new options in the learning process of an agent. We use the MuJoCo environment to model continuous action tasks. 

Results on these tasks yield interesting results which imply that the choice of the deliberation cost value depends highly on the reward model of the environment.

{% include googleDrivePlayer.html id=page.driveId1 %}

{% include googleDrivePlayer.html id=page.driveId2 %}
<!-- <iframe src="https://drive.google.com/file/d/1zIUueEWeAM3lvAZ5S-qXTM8y8EMkZL5X/preview" width="640" height="480"></iframe>

<iframe src="https://drive.google.com/file/d/1IIA071X28tqwY61HPKJ3ZkcJuoFHlpGX/preview" width="640" height="480"></iframe> -->

You can find the entire project here:
[Code](https://github.com/ShubhamGondane/cse574 "Github link")

And the report here:
[Report](https://shubhamgondane.github.io/files/ppoc.pdf "Project Report")

#### References

1. **Klissarov, M., Bacon, P., Harb, J., Precup, D. (2017). Learnings Options End-to-End for Continuous Action Tasks. ArXiv.org, ArXiv.org, Nov 30, 2017.**
